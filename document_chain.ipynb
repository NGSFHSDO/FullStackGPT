{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff449bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'`git status` is a Git command that shows the current status of the working directory and staging area. It displays information about tracked, untracked, modified, and staged files in the repository. This command helps users understand what changes have been made and what needs to be committed.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stuff Documents Chain(off-the-shelf) => 모든 document를 prompt에 넣음.\n",
    "### 다양한 vector store, Documents chain을 조합해서 잘 맞는것을 찾자.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # refine, map_reduce, map_rerank\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "chain.run(\"What does 'git status' do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85861a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='보안은 fireStore, storage의 규칙설정이나 google cloud api credential에서 설정해야 합니다.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stuff Documents Chain(LCEL)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"보안은 어디서 설정해야하지?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='보안은 fireStore, storage의 규칙설정이나 google cloud api credential에서 설정해야합니다.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MapReduce Documents Chain(LCEL)\n",
    "# 원래 document chunk중 쓸만한거만 남기고, 개량된 document를 prompt에 전달하는것.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    results = []\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\"context\": document.page_content, \"question\": question}).content\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs) # => RunnableLambda로 아무 함수나 실행 ㄱㄴ\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "chain.invoke(\"보안은 어디서 설정해야하지?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
