{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff449bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'`git status` is a Git command that shows the current status of the working directory and staging area. It displays information about tracked, untracked, modified, and staged files in the repository. This command helps users understand what changes have been made and what needs to be committed.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stuff Documents Chain(off-the-shelf) => 모든 document를 prompt에 넣음.\n",
    "### 다양한 vector store, Documents chain을 조합해서 잘 맞는것을 찾자.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # refine, map_reduce, map_rerank\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "chain.run(\"What does 'git status' do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85861a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='보안은 fireStore, storage의 규칙설정이나 google cloud api credential에서 설정해야 합니다.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stuff Documents Chain(LCEL)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"보안은 어디서 설정해야하지?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87fef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 688, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='보안은 fireStore, storage의 규칙설정이나 google cloud api credential에서 설정해야합니다.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MapReduce Documents Chain(LCEL)\n",
    "# 원래 document chunk중 쓸만한거만 남기고, 개량된 document를 prompt에 전달하는것.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma  # FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    results = []\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\"context\": document.page_content, \"question\": question}).content\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "chain.invoke(\"보안은 어디서 설정해야하지?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e880a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MapReduce Documents Chain (LCEL)\n",
    "# 원래 document chunk 중 질문과 관련 있는 부분만 추출(Map),\n",
    "# 그 추출된 부분을 모아 최종 답을 생성(Reduce)하는 RAG 체인 예시\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# 1) LLM 세팅\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 2) 문서 로드 & 청크화\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./meeeemooo.md\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# 3) 임베딩 + 캐시 + 벡터스토어\n",
    "embedder = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cache_embedder)\n",
    "\n",
    "# retriever \n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# 4) Map 단계 프롬프트: 각 청크에서 질문 관련 구절만 추출\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text verbatim. \n",
    "            If there is no relevant text, return nothing.\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# 5) Map 함수: retriever 결과 문서들을 순회하며 관련 구절만 뽑아 합침\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]      # retriever가 반환한 상위 k개 청크\n",
    "    question = inputs[\"question\"]        # 사용자 질문\n",
    "    results = []\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\"context\": document.page_content, \"question\": question}).content\n",
    "        results.append(result)\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "# 6) Map 체인: 질문 → retriever + 질문 그대로 전달 → map_docs\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "# 7) Reduce 단계 프롬프트: Map에서 뽑힌 부분을 모아 최종 답 작성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 8) 최종 체인 = Map + Reduce\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "# 9) 실행 예시\n",
    "print(chain.invoke(\"보안은 어디서 설정해야하지?\").content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
